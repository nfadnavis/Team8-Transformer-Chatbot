{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Step 1: Mount Google Drive and Load the Data\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Updated paths to your files\n",
        "movie_lines_file = '/content/drive/My Drive/Collab_finalprj/movie_lines.txt'\n",
        "movie_conversations_file = '/content/drive/My Drive/Collab_finalprj/movie_conversations.txt'\n",
        "\n",
        "# Load the movie lines data with encoding handling\n",
        "movie_lines = pd.read_csv(movie_lines_file, sep=r\" \\+\\+\\+\\$\\+\\+\\+ \", header=None, engine=\"python\",\n",
        "                          names=[\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"], encoding=\"ISO-8859-1\")\n",
        "\n",
        "# Load the movie conversations data with encoding handling\n",
        "movie_conversations = pd.read_csv(movie_conversations_file, sep=r\" \\+\\+\\+\\$\\+\\+\\+ \", header=None, engine=\"python\",\n",
        "                                  names=[\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"], encoding=\"ISO-8859-1\")  # Corrected this line\n",
        "\n",
        "# Step 2: Clean Text and Tokenize\n",
        "def clean_text(text):\n",
        "    return text.lower().strip() if isinstance(text, str) else \"\"\n",
        "\n",
        "# Apply cleaning to movie lines\n",
        "movie_lines['cleaned_text'] = movie_lines['text'].apply(clean_text)\n",
        "\n",
        "# Tokenized movie lines (using simple space-based tokenization)\n",
        "movie_lines['tokens'] = movie_lines['cleaned_text'].apply(lambda x: x.split())\n",
        "\n",
        "# Display sample results\n",
        "print(\"Sample movie lines:\")\n",
        "print(movie_lines[['lineID', 'characterID', 'movieID', 'character', 'text']].head())\n",
        "print(\"\\nSample movie conversations:\")\n",
        "print(movie_conversations[['character1ID', 'character2ID', 'movieID', 'utteranceIDs']].head())\n",
        "print(\"\\nCleaned movie lines:\")\n",
        "print(movie_lines[['text', 'cleaned_text']].head())\n",
        "print(\"\\nTokenized movie lines:\")\n",
        "print(movie_lines[['cleaned_text', 'tokens']].head())\n",
        "\n",
        "# Step 3: Prepare the Sequences for the Seq2Seq Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "def prepare_data_for_seq2seq(movie_lines, movie_conversations):\n",
        "    # Create a dictionary to map lineIDs to text\n",
        "    line_to_text = dict(zip(movie_lines['lineID'], movie_lines['cleaned_text']))\n",
        "\n",
        "    input_texts = []\n",
        "    target_texts = []\n",
        "\n",
        "    # Iterate through the conversations\n",
        "    for _, row in movie_conversations.iterrows():\n",
        "        utterance_ids = eval(row['utteranceIDs'])  # Convert string representation of list back to list\n",
        "        for i in range(len(utterance_ids) - 1):\n",
        "            input_line = line_to_text.get(utterance_ids[i], \"\")\n",
        "            target_line = line_to_text.get(utterance_ids[i + 1], \"\")\n",
        "            if input_line and target_line:\n",
        "                input_texts.append(input_line)\n",
        "                target_texts.append(target_line)\n",
        "\n",
        "    # Tokenize the input and target texts\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(input_texts + target_texts)\n",
        "\n",
        "    input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
        "    target_sequences = tokenizer.texts_to_sequences(target_texts)\n",
        "\n",
        "    # Pad sequences to the same length\n",
        "    max_sequence_length = max(len(seq) for seq in input_sequences)\n",
        "    input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')\n",
        "    target_sequences = pad_sequences(target_sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "    return input_sequences, target_sequences, tokenizer\n",
        "\n",
        "# Prepare the data for Seq2Seq\n",
        "input_sequences, target_sequences, tokenizer = prepare_data_for_seq2seq(movie_lines, movie_conversations)\n",
        "\n",
        "# Get the vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(\"Vocabulary size:\", vocab_size)\n",
        "print(\"Sample input sequence:\", input_sequences[0])\n",
        "print(\"Sample target sequence:\", target_sequences[0])\n",
        "\n",
        "# Step 4: Define the Seq2Seq model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "\n",
        "def define_seq2seq_model(vocab_size, max_sequence_length):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(max_sequence_length,))\n",
        "    encoder_embedding = Embedding(vocab_size, 256)(encoder_inputs)\n",
        "    encoder_lstm, state_h, state_c = LSTM(256, return_state=True)(encoder_embedding)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(max_sequence_length,))\n",
        "    decoder_embedding = Embedding(vocab_size, 256)(decoder_inputs)\n",
        "    decoder_lstm = LSTM(256, return_sequences=True, return_state=False)(decoder_embedding, initial_state=encoder_states)\n",
        "    decoder_dense = Dense(vocab_size, activation='softmax')(decoder_lstm)\n",
        "\n",
        "    # Seq2Seq model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_dense)\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Step 5: Train the Seq2Seq Model\n",
        "# Instantiate the model\n",
        "max_sequence_length = input_sequences.shape[1]\n",
        "seq2seq_model = define_seq2seq_model(vocab_size, max_sequence_length)\n",
        "\n",
        "# Print the model summary\n",
        "seq2seq_model.summary()\n",
        "\n",
        "# Reshape the target_sequences to match the output shape for training\n",
        "target_sequences = target_sequences.reshape(target_sequences.shape[0], target_sequences.shape[1], 1)\n",
        "\n",
        "# Train the model (adjust epochs and batch_size for memory handling)\n",
        "history = seq2seq_model.fit([input_sequences, target_sequences], target_sequences, epochs=10, batch_size=32)  # Reduced batch size\n",
        "\n",
        "# Save the model after training\n",
        "seq2seq_model.save('seq2seq_movie_chatbot.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZJ3Jm1Dx_Ksq",
        "outputId": "e4ab3c89-b455-4afc-876c-456b009c5b09"
      },
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Sample movie lines:\n",
            "  lineID characterID movieID character          text\n",
            "0  L1045          u0      m0    BIANCA  They do not!\n",
            "1  L1044          u2      m0   CAMERON   They do to!\n",
            "2   L985          u0      m0    BIANCA    I hope so.\n",
            "3   L984          u2      m0   CAMERON     She okay?\n",
            "4   L925          u0      m0    BIANCA     Let's go.\n",
            "\n",
            "Sample movie conversations:\n",
            "  character1ID character2ID movieID                      utteranceIDs\n",
            "0           u0           u2      m0  ['L194', 'L195', 'L196', 'L197']\n",
            "1           u0           u2      m0                  ['L198', 'L199']\n",
            "2           u0           u2      m0  ['L200', 'L201', 'L202', 'L203']\n",
            "3           u0           u2      m0          ['L204', 'L205', 'L206']\n",
            "4           u0           u2      m0                  ['L207', 'L208']\n",
            "\n",
            "Cleaned movie lines:\n",
            "           text  cleaned_text\n",
            "0  They do not!  they do not!\n",
            "1   They do to!   they do to!\n",
            "2    I hope so.    i hope so.\n",
            "3     She okay?     she okay?\n",
            "4     Let's go.     let's go.\n",
            "\n",
            "Tokenized movie lines:\n",
            "   cleaned_text            tokens\n",
            "0  they do not!  [they, do, not!]\n",
            "1   they do to!   [they, do, to!]\n",
            "2    i hope so.    [i, hope, so.]\n",
            "3     she okay?      [she, okay?]\n",
            "4     let's go.      [let's, go.]\n",
            "Vocabulary size: 55822\n",
            "Sample input sequence: [   50    26   109    15   988 42153 42154     7  4154  8453    33   418\n",
            "    82  3928 21970   981   501    46    29     3 30044   190     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0]\n",
            "Sample target sequence: [   56     2   145   653   324    32 30045    41    49   115    32     1\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_3             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">14,290,432</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">14,290,432</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]     │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                           │                        │                │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],          │\n",
              "│                           │                        │                │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55822</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">14,346,254</span> │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_3             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m14,290,432\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m14,290,432\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)             │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │        \u001b[38;5;34m525,312\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│                           │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]     │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │        \u001b[38;5;34m525,312\u001b[0m │ embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                           │                        │                │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],          │\n",
              "│                           │                        │                │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m55822\u001b[0m)     │     \u001b[38;5;34m14,346,254\u001b[0m │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,977,742</span> (167.76 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m43,977,742\u001b[0m (167.76 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,977,742</span> (167.76 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m43,977,742\u001b[0m (167.76 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m6916/6916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m873s\u001b[0m 126ms/step - accuracy: 0.9805 - loss: 0.2845\n",
            "Epoch 2/10\n",
            "\u001b[1m6916/6916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m871s\u001b[0m 126ms/step - accuracy: 0.9981 - loss: 0.0175\n",
            "Epoch 3/10\n",
            "\u001b[1m6916/6916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m872s\u001b[0m 126ms/step - accuracy: 0.9992 - loss: 0.0070\n",
            "Epoch 4/10\n",
            "\u001b[1m6916/6916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m872s\u001b[0m 126ms/step - accuracy: 0.9996 - loss: 0.0032\n",
            "Epoch 5/10\n",
            "\u001b[1m6916/6916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m873s\u001b[0m 126ms/step - accuracy: 0.9998 - loss: 0.0019\n",
            "Epoch 6/10\n",
            "\u001b[1m6916/6916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m873s\u001b[0m 126ms/step - accuracy: 0.9999 - loss: 0.0012\n",
            "Epoch 7/10\n",
            "\u001b[1m6916/6916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m873s\u001b[0m 126ms/step - accuracy: 1.0000 - loss: 3.3280e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m6916/6916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m873s\u001b[0m 126ms/step - accuracy: 1.0000 - loss: 1.0395e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m6916/6916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m873s\u001b[0m 126ms/step - accuracy: 1.0000 - loss: 4.0038e-05\n",
            "Epoch 10/10\n",
            "\u001b[1m6916/6916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m872s\u001b[0m 126ms/step - accuracy: 1.0000 - loss: 1.5121e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    }
  ]
}